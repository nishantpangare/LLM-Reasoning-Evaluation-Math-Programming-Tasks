{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69cff7f",
   "metadata": {},
   "source": [
    "Reasoning with LLMs: Chain-of-Thought [CoT] & Verifier on Math and Code Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c7973",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "This project implements a compact reasoning pipeline for Large Language Models (LLMs) using Chain-of-Thought prompting, self-consistency sampling, and lightweight verifiers. A small hand-crafted dataset of mathematical and programming tasks is used to evaluate how well LLMs reason step by step, where they fail, and how verifiers can improve reliability. The goal is to demonstrate and understand transparent and trustworthy reasoning methods aligned with current research directions in efficient LLM reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca37e6",
   "metadata": {},
   "source": [
    "Short Dataset (Math and Programming Samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738ea739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math dataset\n",
    "math_dataset = [\n",
    "    {\"id\": 1, \"question\": \"If Lina has 5 apples and buys 7 more, how many does she have?\", \"answer\": \"12\"},\n",
    "    {\"id\": 2, \"question\": \"A train travels 40 km in 2 hours. What is its speed in km/h?\", \"answer\": \"20\"},\n",
    "    {\"id\": 3, \"question\": \"What is 15 × 6?\", \"answer\": \"90\"},\n",
    "    {\"id\": 4, \"question\": \"If x + 3 = 10, what is x?\", \"answer\": \"7\"},\n",
    "    {\"id\": 5, \"question\": \"What is 144 ÷ 12?\", \"answer\": \"12\"},\n",
    "    {\"id\": 6, \"question\": \"If a rectangle has sides 4 and 9, what is its area?\", \"answer\": \"36\"},\n",
    "    {\"id\": 7, \"question\": \"Add 123 and 456.\", \"answer\": \"579\"},\n",
    "    {\"id\": 8, \"question\": \"What is 2 to the power 8?\", \"answer\": \"256\"},\n",
    "    {\"id\": 9, \"question\": \"If a = 3 and b = 4, what is a^2 + b^2?\", \"answer\": \"25\"},\n",
    "    {\"id\": 10, \"question\": \"What is (7 × 8) - 10?\", \"answer\": \"46\"},\n",
    "]\n",
    "\n",
    "# Programming dataset\n",
    "programming_dataset = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"task\": \"Write a Python function is_prime(n) that returns True if n is prime.\",\n",
    "        \"function_name\": \"is_prime\",\n",
    "        \"tests\": [\n",
    "            (\"is_prime(2)\", True),\n",
    "            (\"is_prime(15)\", False),\n",
    "            (\"is_prime(17)\", True),\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"task\": \"Write a Python function factorial(n) returning factorial of n.\",\n",
    "        \"function_name\": \"factorial\",\n",
    "        \"tests\": [\n",
    "            (\"factorial(0)\", 1),\n",
    "            (\"factorial(5)\", 120),\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"task\": \"Write a Python function reverse_string(s) returning the reversed string.\",\n",
    "        \"function_name\": \"reverse_string\",\n",
    "        \"tests\": [\n",
    "            (\"reverse_string('abc')\", \"cba\"),\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"task\": \"Write a Python function sum_list(xs) returning the sum of numbers in a list.\",\n",
    "        \"function_name\": \"sum_list\",\n",
    "        \"tests\": [\n",
    "            (\"sum_list([1,2,3])\", 6),\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"task\": \"Write a Python function fibonacci(n) returning nth Fibonacci number (0-indexed).\",\n",
    "        \"function_name\": \"fibonacci\",\n",
    "        \"tests\": [\n",
    "            (\"fibonacci(0)\", 0),\n",
    "            (\"fibonacci(5)\", 5),\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94f5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"math\": math_dataset,\n",
    "    \"programming\": programming_dataset\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560dbb0",
   "metadata": {},
   "source": [
    "Load The Model Using Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08a0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2fee09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6624e2a352e4afdb63b3fe2e73dbf87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-xl\")\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23044f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 * 8 = 96. The final answer: 96.\n"
     ]
    }
   ],
   "source": [
    "# Trial Run\n",
    "\n",
    "prompt = \"Question: What is 12 * 8?\\nLet's think step by step and calculate the final numeric answer only.\"\n",
    "output = generator(prompt, max_new_tokens=50)\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc244f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-thought CoT style math prompt\n",
    "def cot_prompt_math(question):\n",
    "    return f\"Solve step by step and give the final numeric answer.\\nQuestion: {question}\"\n",
    "\n",
    "# Programming prompt\n",
    "def code_prompt(task):\n",
    "    return f\"Task: {task}\\nWrite a Python function. Only output the function code.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d493a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_and_eval(text):\n",
    "    # Look for 'final answer'\n",
    "    m = re.search(r'Final\\s*Answer[:\\s]*([-+]?\\d+\\.?\\d*)', text, re.IGNORECASE)\n",
    "    if m: return m.group(1)\n",
    "    \n",
    "    # Evaluate simple arithmetic expressions\n",
    "    exprs = re.findall(r'[-+]?\\d+\\s*[\\+\\-\\*/]\\s*[-+]?\\d+', text)\n",
    "    if exprs:\n",
    "        try:\n",
    "            return str(eval(exprs[-1]))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Last number fallback\n",
    "    nums = re.findall(r\"[-+]?\\d+\\.?\\d*\", text)\n",
    "    return nums[-1] if nums else None\n",
    "\n",
    "def extract_function(text):\n",
    "    idx = text.find(\"def \")\n",
    "    if idx == -1: return None\n",
    "    return text[idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "199b4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_math(pred, truth):\n",
    "    try:\n",
    "        return float(pred) == float(truth)\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "\n",
    "import multiprocessing, traceback\n",
    "\n",
    "def _worker_run(code, test_cases, q):\n",
    "    try:\n",
    "        local_env = {}\n",
    "        exec(code, {}, local_env)\n",
    "        funcs = [v for v in local_env.values() if callable(v)]\n",
    "        if not funcs:\n",
    "            q.put((False, \"No function found\"))\n",
    "            return\n",
    "        func = funcs[0]\n",
    "        for expr, expected in test_cases:\n",
    "            if eval(expr, {}, local_env) != expected:\n",
    "                q.put((False, f\"Failed case {expr} != {expected}\"))\n",
    "                return\n",
    "        q.put((True, \"ok\"))\n",
    "    except:\n",
    "        q.put((False, traceback.format_exc()))\n",
    "\n",
    "def run_unit_tests(func_code, test_cases):\n",
    "    try:\n",
    "        local_env = {}\n",
    "        exec(func_code, {}, local_env)\n",
    "        # Get the first function defined\n",
    "        func = [v for v in local_env.values() if callable(v)][0]\n",
    "        for expr, expected in test_cases:\n",
    "            if eval(expr, {}, local_env) != expected:\n",
    "                return False, f\"Failed case {expr} != {expected}\"\n",
    "        return True, \"ok\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14dccef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def generate_answers(prompt, n=5, max_new_tokens=120, temp=0.3):\n",
    "    res = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        num_return_sequences=n,\n",
    "        num_beams=1\n",
    "    )\n",
    "    return [r[\"generated_text\"] for r in res]\n",
    "\n",
    "def majority_vote(ans_list):\n",
    "    cleaned = [a for a in ans_list if a is not None]\n",
    "    if not cleaned: return None\n",
    "    counts = Counter(cleaned)\n",
    "    return counts.most_common(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc4e138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:08<00:00, 30.86s/it]\n",
      "100%|██████████| 5/5 [1:04:40<00:00, 776.19s/it] \n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "math_results = []\n",
    "\n",
    "for item in tqdm(dataset[\"math\"]):\n",
    "    prompt = cot_prompt_math(item[\"question\"])\n",
    "    \n",
    "    # Greedy\n",
    "    greedy_out = generator(prompt, max_new_tokens=100, do_sample=False)[0][\"generated_text\"]\n",
    "    greedy_ans = extract_and_eval(greedy_out)\n",
    "    \n",
    "    # Self-consistency\n",
    "    sampled_outs = generate_answers(prompt, n=5, max_new_tokens=100, temp=0.3)\n",
    "    sampled_ans = [extract_and_eval(t) for t in sampled_outs]\n",
    "    majority = majority_vote(sampled_ans)\n",
    "    \n",
    "    math_results.append({\n",
    "        \"id\": item[\"id\"],\n",
    "        \"question\": item[\"question\"],\n",
    "        \"truth\": item[\"answer\"],\n",
    "        \"greedy_text\": greedy_out,\n",
    "        \"greedy\": greedy_ans,\n",
    "        \"sampled_texts\": sampled_outs,\n",
    "        \"sampled_answers\": sampled_ans,\n",
    "        \"majority\": majority,\n",
    "        \"greedy_correct\": verify_math(greedy_ans, item[\"answer\"]),\n",
    "        \"majority_correct\": verify_math(majority, item[\"answer\"])\n",
    "    })\n",
    "\n",
    "# Save CSV\n",
    "with open(\"math_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=math_results[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(math_results)\n",
    "\n",
    "\n",
    "prog_results = []\n",
    "\n",
    "for item in tqdm(dataset[\"programming\"]):\n",
    "    prompt = code_prompt(item[\"task\"])\n",
    "    \n",
    "    # Greedy\n",
    "    greedy_out = generator(prompt, max_new_tokens=200, do_sample=False)[0][\"generated_text\"]\n",
    "    greedy_func = extract_function(greedy_out)\n",
    "    greedy_ok, greedy_msg = (False, None)\n",
    "    if greedy_func:\n",
    "        greedy_ok, greedy_msg = run_unit_tests(greedy_func, item[\"tests\"])\n",
    "    \n",
    "    # Self-consistency\n",
    "    sampled_outs = generate_answers(prompt, n=5, max_new_tokens=200, temp=0.7)\n",
    "    sampled_funcs = [extract_function(t) for t in sampled_outs if extract_function(t)]\n",
    "    passing_funcs = []\n",
    "    for f in sampled_funcs:\n",
    "        ok, msg = run_unit_tests(f, item[\"tests\"])\n",
    "        if ok: passing_funcs.append(f)\n",
    "    \n",
    "    majority_func = None\n",
    "    majority_ok = False\n",
    "    if passing_funcs:\n",
    "        majority_func = Counter(passing_funcs).most_common(1)[0][0]\n",
    "        majority_ok = True\n",
    "    \n",
    "    prog_results.append({\n",
    "        \"id\": item[\"id\"],\n",
    "        \"task\": item[\"task\"],\n",
    "        \"greedy_function\": greedy_func,\n",
    "        \"greedy_ok\": greedy_ok,\n",
    "        \"greedy_msg\": greedy_msg,\n",
    "        \"sampled_texts\": sampled_outs,\n",
    "        \"majority_function\": majority_func,\n",
    "        \"majority_ok\": majority_ok\n",
    "    })\n",
    "\n",
    "# Save CSV\n",
    "with open(\"programming_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=prog_results[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(prog_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
